{
 "metadata": {
  "name": "",
  "signature": "sha256:1e8039c09dd3a1f4b7a45ce10fb1aba0a2e4529273e906264120b71019db38db"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_keyphrase_columns(\n",
      "        keyphrases,\n",
      "        indf,\n",
      "        keyphrase_col='common_chunks',\n",
      "        read_col='article_chunks',\n",
      "        make_keyphrase_cols=True\n",
      "        ):\n",
      "    \"\"\"\n",
      "    takes a list of keyphrases, a dataframe, and a column name where\n",
      "    the rows keyphrases are kept in a list. \n",
      "    \n",
      "    returns a data frame(index is same as indf) with a row for each document and a column for each keyphrase.\n",
      "    1 if keyphrase is in document, 0 otherwise. \n",
      "    \"\"\"\n",
      "    keyphrases=set(keyphrases)\n",
      "    df_chunks=pd.DataFrame(index=indf.index)\n",
      "    df_chunks[keyphrase_col]=indf[read_col].apply(lambda r: set(r)&keyphrases).\\\n",
      "        apply(lambda r: set([c.encode('utf8', 'ignore') for c in r]))\n",
      "    \n",
      "    if make_keyphrase_cols:\n",
      "        for i,v in enumerate(keyphrases):\n",
      "            df_chunks[v.encode('utf8', 'ignore')]=df_chunks.common_chunks.apply(lambda r: 1 if v in r else 0)\n",
      "    \n",
      "    return df_chunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_overlaps(keyphrases, indf, keyphrase_col='common_chunks', as_percent=False):\n",
      "    \"\"\"\n",
      "    takes a list of keyphrases, a dataframe, and a column name where\n",
      "    the rows keyphrases are kept in a list. \n",
      "    \n",
      "    returns a df where each keyphrase has a row and column. cells indicate number\n",
      "    of rows from indf that had both keyphrases in keyphrase col. \n",
      "    \n",
      "    If as percent is true then the df values are the percentage of times documents\n",
      "    containing the row label contain the column label. \n",
      "    \"\"\"\n",
      "    df_chunks=get_keyphrase_columns(keyphrases, indf, keyphrase_col)\n",
      "    \n",
      "    chunk_overlap = pd.DataFrame(index=df_chunks.columns[1:])\n",
      "    \n",
      "    for i,v in enumerate(keyphrases):\n",
      "        v = v.encode('utf8', 'ignore')\n",
      "        chunk_overlap[v]=df_chunks[df_chunks[v]==1][df_chunks.columns[1:]].sum()\n",
      "    \n",
      "    if as_percent:\n",
      "        return chunk_overlap.apply(lambda r: r/max(r), axis=1)\n",
      "    else:\n",
      "        return chunk_overlap"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def check_sets(inset, filter_sets, min_matches=1):\n",
      "    \"checks if any of the filter sets contain inset\"\n",
      "    m=0\n",
      "    for s in filter_sets:\n",
      "        if s>=inset:\n",
      "            m+=1\n",
      "            if m>=min_matches:\n",
      "                return True\n",
      "    \n",
      "    return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_groups(indf, doc_percents=True, set_filters=None, min_matches=1):\n",
      "    \"\"\"\n",
      "    Returns groups from df of key phrase overlaps.\n",
      "    \n",
      "    doc_percents - Whether score should be calculated using\n",
      "                    #term1/#(term1&term2)\n",
      "                    otherwise it is just a constant/#(term1&term2)\n",
      "    \"\"\"\n",
      "        \n",
      "    if doc_percents:\n",
      "        overlap_freqs=indf.apply(lambda r: r/max(r), axis=1)\n",
      "    else:\n",
      "        # divide by const to get most frequent coccurences\n",
      "        overlap_freqs=indf.apply(lambda r: r/100, axis=1)\n",
      "    \n",
      "    # create list of relationships between keyphrases\n",
      "    relationships = []\n",
      "    for i, c1 in enumerate(overlap_freqs.columns):\n",
      "        for c2 in overlap_freqs.columns[i:]:\n",
      "            if c1==c2:\n",
      "                continue\n",
      "            val=(c1,c2,overlap_freqs.loc[c1][[c2]][0], overlap_freqs.loc[c2][[c1]][0])\n",
      "\n",
      "            relationships.append(val)\n",
      "    \n",
      "    # sort relationships by weaker of pair,highest first\n",
      "    relationships.sort(key=lambda r: min([r[2], r[3]]), reverse=True)\n",
      "    \n",
      "    \n",
      "    # build groups\n",
      "    term_groups = {}\n",
      "    groups=[]\n",
      "    \n",
      "    for r in relationships:\n",
      "        if r[0] in term_groups.keys() and r[1] in term_groups.keys():\n",
      "            # check if both terms have been grouped\n",
      "            continue\n",
      "            \n",
      "        elif r[0] not in term_groups.keys() and r[1] not in term_groups.keys():\n",
      "            # create new group if neither term groupd\n",
      "            g = len(groups)\n",
      "            groups.append([r[0], r[1]])\n",
      "            term_groups[r[0]]=g\n",
      "            term_groups[r[1]]=g\n",
      "        \n",
      "        # next 2: add ungrouped term to other group's term\n",
      "        elif r[0] not in term_groups.keys():\n",
      "            g=groups[term_groups[r[1]]]\n",
      "            if set_filters is not None and not check_sets(set(g+[r[0]]), set_filters, min_matches):\n",
      "                continue\n",
      "            g.append(r[0])\n",
      "            term_groups[r[0]]=term_groups[r[1]]\n",
      "        \n",
      "        elif r[1] not in term_groups.keys():\n",
      "            g=groups[term_groups[r[0]]]\n",
      "            if set_filters is not None and not check_sets(set(g+[r[1]]), set_filters, min_matches):\n",
      "                continue\n",
      "            g.append(r[1])\n",
      "            term_groups[r[1]]=term_groups[r[0]]\n",
      "            \n",
      "        else:\n",
      "            continue\n",
      "        \n",
      "    return groups"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_grouped_df(indf, groups, ordering='ordering', group='group'):\n",
      "    \"\"\"\n",
      "    returns a df with rows and columns sorted in the order groups and with a group column\n",
      "    \"\"\"\n",
      "    column_order=[t for g in groups for t in g]\n",
      "    row_groups={t:g for g,l in enumerate(groups) for t in l}\n",
      "    \n",
      "    df_out = indf.copy(deep=False)\n",
      "    df_out[ordering]=pd.Series({k:v for v,k in enumerate(column_order)})\n",
      "    df_out[group]=pd.Series(row_groups)\n",
      "    \n",
      "    df_out.sort(ordering, inplace=True)\n",
      "    show = [ordering, group]+column_order\n",
      "    return df_out[show]\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}