{
 "metadata": {
  "name": "",
  "signature": "sha256:4e23230497f0cd0376b471db5dd7e5a5c08abc67c30ccead5705c1805c3120c2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import path\n",
      "import cPickle as pickle\n",
      "\n",
      "import nltk, string\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.collocations import *\n",
      "import codecs, os, json, glob\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import wordnet as wn\n",
      "from pandas import DataFrame\n",
      "import pandas as pd\n",
      "\n",
      "import chunking_util\n",
      "import grouping_util"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def RegTokenizeWords(wordlist):\n",
      "    pattern = r\"([A-Z]\\.)+|\\w+([-']\\w+)*|\\$?\\d+(\\.\\d+)?%?|\\.\\.\\.|[][.,;\\\"'?():-_`]\"\n",
      "    return [word for word in nltk.regexp_tokenize(wordlist,pattern)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def CreateNgramsOnTitle(articles_df):\n",
      "    title_words = []\n",
      "    for art in articles_df['tokenized_title2']:\n",
      "        title_words.extend([word.replace(u\"\\u2018\", \"\").replace(u\"\\u2019\", \"\").replace(u\"\\u201c\",'').replace(u\"\\u201d\", '') for word in  art])\n",
      "\n",
      "    title_words_noStop_noPunct = [word for word in title_words if word.lower() not in stopwords.words('english') and word not in string.punctuation] \n",
      "\n",
      "    freqD3 = nltk.FreqDist(nltk.bigrams(title_words_noStop_noPunct))\n",
      "    top_bigrams = freqD3.keys()[:50]\n",
      "\n",
      "    freqD4 = nltk.FreqDist(nltk.trigrams(title_words_noStop_noPunct))\n",
      "    top_trigrams = freqD4.keys()[:50]\n",
      "\n",
      "    freqD5 = nltk.FreqDist(nltk.ngrams(title_words_noStop_noPunct,4))\n",
      "    top_ngrams = freqD5.keys()[:50]\n",
      "    \n",
      "    title_df = DataFrame({\"top_bigrams\":top_bigrams, \"top_trigrams\":top_trigrams, \"top_ngrams\":top_ngrams})\n",
      "    \n",
      "    return title_df\n",
      "\n",
      "def getTrisAndNgramsForBiGrams(top_bigrams,top_trigrams,top_ngrams):\n",
      "    finallist ={}\n",
      "    for b1,b2 in top_bigrams[:15]:\n",
      "        finallist[(b1,b2)] = []\n",
      "        for t1,t2,t3 in top_trigrams[:30]:\n",
      "            added=False\n",
      "            \n",
      "            if b1.lower() in [t1.lower(),t2.lower(),t3.lower()] and b2.lower() in [t1.lower(),t2.lower(),t3.lower()]:\n",
      "                \n",
      "                for ngram in top_ngrams[:30]:\n",
      "                    ngramstring = \" \".join(ngram)\n",
      "                    if t1.lower() in ngramstring.lower() and t2.lower() in ngramstring.lower() and t3.lower() in ngramstring.lower():\n",
      "                        finallist[(b1,b2)].append(ngram)\n",
      "                        added = True\n",
      "                if added == False:        \n",
      "                    finallist[(b1,b2)].append((t1,t2,t3))\n",
      "    return finallist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def GetSentencesBasedOnTerms(articles_df,keywords):\n",
      "    \n",
      "    finalsents ={}\n",
      "    #list of keywords\n",
      "    for key in keywords.keys():\n",
      "        finalsents[key] =[]\n",
      "        #article\n",
      "        for art in articles_df[\"sent_tokenized\"]:\n",
      "            #sentences\n",
      "            for sent in art:\n",
      "                #handling len == 0\n",
      "                if len(keywords[key])== 0:\n",
      "                    if key[0].lower() in sent.lower() and key[1].lower() in sent.lower():\n",
      "                        finalsents[key].append(RegTokenizeWords(sent))\n",
      "                else:\n",
      "                    #terms for each keyword\n",
      "                    for term in keywords[key]:\n",
      "                        if len(term) == 3:\n",
      "                            if term[0].lower() in sent.lower() and term[1].lower() in sent.lower() and term[2].lower() in sent.lower():\n",
      "                                finalsents[key].append(RegTokenizeWords(sent))\n",
      "                                break\n",
      "                        if len(term) == 4:\n",
      "                            if term[0].lower() in sent.lower() and term[1].lower() in sent.lower() and term[2].lower() in sent.lower() and term[3].lower() in sent.lower():\n",
      "                                finalsents[key].append(RegTokenizeWords(sent))\n",
      "                                break\n",
      "            #Trying to handle cases where the tri/n grams cannot be found in sentences\n",
      "            if len(finalsents[key]) == 0:\n",
      "                for art in articles_df[\"sent_tokenized\"]:\n",
      "                    for sent in art:\n",
      "                        if key[0].lower() in sent.lower() and key[1].lower() in sent.lower():\n",
      "                            finalsents[key].append(RegTokenizeWords(sent))\n",
      "    return finalsents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# finalsents = GetSentencesBasedOnTerms(articles_df)\n",
      "    \n",
      "def GetFinalListOfChunks(finalsents):\n",
      "    fc =[]\n",
      "    chunksByBigram =  GetChunksForEachTopic(finalsents,parseImportantNps)\n",
      "    for key in chunksByBigram.keys():\n",
      "        fd = nltk.FreqDist(chunksByBigram[key])\n",
      "        fc.extend([key for key,val in fd.items()[:10]])\n",
      "    _2n3, _3n4 = getPhraseLengths(fc)\n",
      "    return mergeTerms2Lists(_2n3.keys(),_3n4.keys()).keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#break chunks into groups by length\n",
      "def getPhraseLengths(chunkfd):\n",
      "    lenMore = []\n",
      "    len3 = []\n",
      "    len2 = []\n",
      "    len1 = []\n",
      "    combined = {}\n",
      "    \n",
      "    for c in chunkfd:\n",
      "        if len(c.split()) > 3:\n",
      "            lenMore.append(c)\n",
      "        if len(c.split()) == 3:\n",
      "            len3.append(c)\n",
      "        if len(c.split()) == 2:\n",
      "            len2.append(c)\n",
      "        if len(c.split()) == 1:\n",
      "            len1.append(c)\n",
      "            \n",
      "    len1 = mergeTerms(len1)\n",
      "    len2 = mergeTerms(len2)\n",
      "    len3 = mergeTerms(len3)\n",
      "    lenMore = mergeTerms(lenMore)\n",
      "    \n",
      "    return mergeTerms2Lists(len2, len3), mergeTerms2Lists(len3, lenMore)\n",
      " \n",
      "    \n",
      "#merge ngrams into same length other ngrams\n",
      "def mergeTerms(list1):\n",
      "    \n",
      "    results = []\n",
      "    list2 = set(list1)\n",
      "    \n",
      "    for i in list2: \n",
      "        flag = True\n",
      "        for j in list2:\n",
      "            if i.lower() in j.lower() and i != j:\n",
      "                results.append(j)\n",
      "                flag = False\n",
      "                \n",
      "        if flag == True and i not in results: \n",
      "            results.append(i)\n",
      "    \n",
      "    return set(results)\n",
      "\n",
      "\n",
      "#merge shorter length terms into longer length terms\n",
      "def mergeTerms2Lists(shortlenlist, longerlenlist):\n",
      "    termMapping = {}\n",
      "    \n",
      "    for j in longerlenlist:\n",
      "        termMapping[j] = []\n",
      "    for i in shortlenlist: \n",
      "        flag = True\n",
      "        for j in longerlenlist:\n",
      "            if i.lower() in j.lower():\n",
      "                termMapping[j].append(i)\n",
      "                flag = False\n",
      "        if flag == True:\n",
      "            termMapping[i] = []\n",
      "    \n",
      "    return termMapping"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Noun phrase chunker\n",
      "#Gets you the noun phrases\n",
      "def SlightlyModifiedChuangChunker(sent):\n",
      "    grammar = \"NP: {<CD>*(((<JJ>|<N.*>)+(<N.*>|<CD>))|<N.*>)}\"\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    result = cp.parse(sent)\n",
      "    return result\n",
      "\n",
      "def parseImportantNps(sent):\n",
      "    grammar = r\"\"\"\n",
      "    N-N: {<DET|CD.*>?<J*|N.*>+<N.*>} # chunk DET/Cardinal w/optional ADJ or N with proper noun\n",
      "           {<NNP.*>+<NNP.*>}             # chunk solo proper nouns only\n",
      "    \"\"\"\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    return cp.parse(sent)\n",
      "\n",
      "#Run the chunker against a Chunker with a specific grammar\n",
      "def ChunkASection(sents,Chunker):\n",
      "    chunkedlist = []\n",
      "    for sent in sents:\n",
      "        chunks =  Chunker(sent)\n",
      "        for chunk in chunks:\n",
      "            if(type(chunk)==type(chunks)):\n",
      "                temp =''\n",
      "                for leaf in chunk.leaves():\n",
      "                    temp += leaf[0]+' '\n",
      "                chunkedlist.append(temp.strip())\n",
      "    return chunkedlist\n",
      "\n",
      "def GetChunksForEachTopic(diction, chunker):\n",
      "    finalChunksByKey = {}\n",
      "    for key in diction.keys():\n",
      "        finalChunksByKey[key] = []\n",
      "        tagged_sents = []\n",
      "        for sent in diction[key]:\n",
      "            tagged_sents.append(nltk.pos_tag(sent))\n",
      "        finalChunksByKey[key].extend(ChunkASection(tagged_sents,chunker))\n",
      "    return finalChunksByKey"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ngram_approach(data_path, min_matches=3):\n",
      "    p = open(data_path, 'r')\n",
      "    df=pickle.load(p)\n",
      "    df['tokenized_title2'] = df['title'].map(RegTokenizeWords)\n",
      "    df_title = CreateNgramsOnTitle(df)\n",
      "    keyterms = getTrisAndNgramsForBiGrams(df_title[\"top_bigrams\"],df_title[\"top_trigrams\"],df_title[\"top_ngrams\"])\n",
      "    sentences = GetSentencesBasedOnTerms(df,keyterms)\n",
      "    keyphrases = GetFinalListOfChunks(sentences)\n",
      "    keyphrase_map = {k.lower():k.lower() for k in keyphrases}\n",
      "    df_overlap=grouping_util.get_overlaps(keyphrase_map, df)\n",
      "    df['common_chunks']=grouping_util.get_keyphrase_columns(keyphrase_map, df, make_keyphrase_cols=False)\n",
      "    groups=grouping_util.get_groups(df_overlap, doc_percents=False,set_filters=df.common_chunks, min_matches=min_matches)\n",
      "    \n",
      "    group_data=[]\n",
      "    \n",
      "    for g in groups:\n",
      "        g_dict={}\n",
      "        g_set=set(g)\n",
      "        g_dict['keyphrases']=g_set\n",
      "        url_list = []\n",
      "        for url in df[df.common_chunks.apply(lambda r: r>=g_set)].url:\n",
      "            url_list.append(url)\n",
      "        \n",
      "        g_dict['urls']=url_list\n",
      "        \n",
      "        group_data.append(g_dict)\n",
      "        \n",
      "    return group_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}