{
 "metadata": {
  "name": "",
  "signature": "sha256:fcc2dc220cc2c9bd441092449e951efe7184400afb5dfd88afcc4b3f4237831d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import path\n",
      "\n",
      "import nltk, string\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.collocations import *\n",
      "import codecs, os, json, glob\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import wordnet as wn\n",
      "from pandas import DataFrame\n",
      "import pandas as pd\n",
      "\n",
      "import file_utils"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dir = path.join(path.curdir, 'data')\n",
      "pickle_dir = path.join(path.curdir, 'pickles')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hash_dirs=[\n",
      "    \"voterID_clean\",\n",
      "    \"Keystonexl\",\n",
      "    \"Cooks\",\n",
      "    \"Ferguson\",\n",
      "    \"GamerGate\"\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for d in hash_dirs:\n",
      "    print d\n",
      "    in_path = path.join(data_dir, d)\n",
      "    out_path = path.join(pickle_dir, \"{}.pkl\".format(d))\n",
      "    df=file_utils.df_from_path(in_path, V=True)\n",
      "    df.to_pickle(out_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "voterID_clean\n",
        "getting data\n",
        "creating dataframe"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tokenizing\n",
        "tagging"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "chunking"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Warning: parsing empty text"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Keystonexl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "getting data\n",
        "creating dataframe"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tokenizing\n",
        "tagging"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "chunking"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Warning: parsing empty text"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Warning: parsing empty text"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cooks"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "getting data\n",
        "creating dataframe"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tokenizing\n",
        "tagging"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "chunking"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Ferguson"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "getting data\n",
        "creating dataframe"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tokenizing\n",
        "tagging"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "chunking"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "GamerGate"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "getting data\n",
        "creating dataframe"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tokenizing\n",
        "tagging"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "chunking"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}