{
 "metadata": {
  "name": "",
  "signature": "sha256:e09da5a840eba730c7eb254dcbd6acaf7de1ecb676fc999ffd78567100872cd1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, string\n",
      "from nltk.corpus import stopwords, brown\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.collocations import *\n",
      "import codecs, os, json, glob\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import wordnet as wn\n",
      "from pandas import DataFrame\n",
      "import pandas as pd\n",
      "from os import path\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_files_to_list(fdir, expression='*[0-9].json'):\n",
      "    \"\"\"\n",
      "    Returns list of files in path matching expression\n",
      "    \"\"\"\n",
      "    filelist = []\n",
      "    for file in glob.glob(path.join(fdir, expression)):\n",
      "        filelist.append(file)\n",
      "    return filelist\n",
      "\n",
      "\n",
      "def clean_unicode(string):\n",
      "    \"Cleans some unicode from string\"\n",
      "    return string.\\\n",
      "        replace(u\"\\u2018\", \"'\").\\\n",
      "        replace(u\"\\u2019\", \"'\").\\\n",
      "        replace(u\"\\u201c\", '\"').\\\n",
      "        replace(u\"\\u201d\", '\"')\n",
      "\n",
      "        \n",
      "def dict_from_path(fdir, expression=\"*.json\"):\n",
      "    \"\"\"Reads files in path matching expression and returns list of dicts containing\n",
      "        title, url, and text\"\"\"\n",
      "    file_list = read_files_to_list(fdir, expression)\n",
      "    out=[]\n",
      "    \n",
      "    for filepath in file_list:\n",
      "        #print filepath\n",
      "        \n",
      "        with open(filepath, 'r') as f:\n",
      "            j = json.load(f)\n",
      "            if 'objects' in j.keys() and 'title' in j['objects'][0]:\n",
      "                #read article and fix unicode\n",
      "                data=j['objects'][0]\n",
      "                url=j['request'][\"pageUrl\"]\n",
      "            else:\n",
      "                # skip if no objects or title\n",
      "                continue\n",
      "            \n",
      "            row={\n",
      "                \"title\":clean_unicode(data['title']),\n",
      "                \"url\":url,\n",
      "                \"article\":clean_unicode(data['text'])\n",
      "                }\n",
      "            out.append(row)\n",
      "                \n",
      "\n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#look for important noun phrase patterns, either nouns preceded by Det+Adj or simply compound nouns\n",
      "def parseImportantNps():\n",
      "    grammar = r\"\"\"\n",
      "    N-N: {<DET|CD.*>?<J*|N.*>+<N.*>} # chunk DET/Cardinal w/optional ADJ or N with proper noun\n",
      "           {<NNP.*>+<NNP.*>}             # chunk solo proper nouns only\n",
      "    \"\"\"\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    return cp, 'N-N'\n",
      "\n",
      "\n",
      "#function to chunk \n",
      "def chunk(tagged, func=parseImportantNps):\n",
      "    chunks = []\n",
      "    leaves = []\n",
      "    cp, cn = func()\n",
      "    for i in tagged:\n",
      "        tree = cp.parse(i)\n",
      "        for subtree in tree.subtrees():\n",
      "            if subtree.node == cn:\n",
      "                leaflist = [leaf[0] for leaf in subtree.leaves()]\n",
      "                chunks.append(subtree.leaves())\n",
      "                leaves.append(' '.join(leaflist))\n",
      "    return leaves\n",
      "\n",
      "def listToTokens(sentenceList):\n",
      "    tokenized_sents = [nltk.word_tokenize(sent.strip('(').strip(')')) for sent in sentenceList]\n",
      "    return tokenized_sents\n",
      "\n",
      "def tagTokens(tokenList):\n",
      "    tagged_sents = [nltk.pos_tag(sent) for sent in tokenList]\n",
      "    return tagged_sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def df_from_path(fdir, expression='*[0-9].json', V=False):\n",
      "    \"\"\"Reads files in path matching expression and returns data frame containing\n",
      "        title, url, and text\"\"\"\n",
      "    \n",
      "    if V:\n",
      "        print \"getting data\"\n",
      "        \n",
      "    data = dict_from_path(fdir, expression)\n",
      "    \n",
      "    if V:\n",
      "        print \"creating dataframe\"\n",
      "        \n",
      "    df=pd.DataFrame.from_dict(data)\n",
      "    df.drop_duplicates(subset='title', inplace=True)\n",
      "    \n",
      "    if V:\n",
      "        print \"tokenizing\"\n",
      "    \n",
      "    df['tokenized_article'] = df['article'].map(nltk.word_tokenize)\n",
      "    df['tokenized_title'] = df['title'].map(nltk.word_tokenize)\n",
      "    df['sent_tokenized'] = df['article'].map(nltk.sent_tokenize)\n",
      "    \n",
      "    if V:\n",
      "        print \"tagging\"\n",
      "    \n",
      "    df['pos_tagged_word_tokenized']=df['sent_tokenized'].map(listToTokens).map(tagTokens)\n",
      "    \n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    }
   ],
   "metadata": {}
  }
 ]
}