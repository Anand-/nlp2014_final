{
 "metadata": {
  "gist_id": "f00815cbe158a89cf35e",
  "name": "",
  "signature": "sha256:5a932a74a2039fa61763c4cb91c3131818ff3c116100caef57f81172cf3a9282"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, string\n",
      "from nltk.corpus import stopwords, brown\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.collocations import *\n",
      "import codecs, os, json, glob\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import wordnet as wn\n",
      "from pandas import DataFrame\n",
      "import pandas as pd\n",
      "wnlemmatizer = nltk.WordNetLemmatizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Baseline Algorithm: Frequency Distribution on Unigrams"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#freq dist function from set of corpus tokens\n",
      "\n",
      "def freqBigramsFromTokens(tokenList):\n",
      "    #modify this list based on corpus\n",
      "    punc = ['.',',', ';', '--', '(', ')', ':', '``', '\\'\\'', '?']\n",
      "    #additional words that should be treated like stopwords after initial assessment of performance\n",
      "    mod_num = ['one','two','three','four','five','first', 'second','third','fourth','fifth', \\\n",
      "               'could','should','would','might','must','will','can']\n",
      "    #concatenate stopwords, punctuation, + special modals/numbers/cardinals list\n",
      "    sw_p = nltk.corpus.stopwords.words('english') + punc + mod_num\n",
      "    wnl_lemmas = [wnlemmatizer.lemmatize(word.lower()) for word in tokenList if word.lower() not in sw_p]\n",
      "    fd = nltk.FreqDist(bigrams(wnl_lemmas))\n",
      "    top_n = fd.items()[:50]\n",
      "    top_n_vals = fd.values()[:50]\n",
      "    i = 0\n",
      "    print '%-20s' % 'WORD PAIR ', '%-20s' % 'BIGRAM COUNT'\n",
      "    for b in top_n:\n",
      "        print '%-12s' % b[0][0], '%-12s' % b[0][1], '%-16d' % top_n_vals[i]\n",
      "        i=i+1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Functions Used for Reading Files from Directory, Tokenizing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to get string from file\n",
      "def loadCorpusAsString(pathToFile):\n",
      "    snippets = open(pathToFile, \"r\")\n",
      "    text_list = snippets.readlines()\n",
      "    snippets.close()\n",
      "    text_string = reduce(lambda x, y: x + ' ' + y.strip('\\t'), text_list)\n",
      "    return text_string\n",
      "\n",
      "\n",
      "#function to get string from file\n",
      "def loadJsonCorpusAsString(path):\n",
      "    snippets = open(path, \"r\")\n",
      "    text_list = json.loads(snippets.readlines())\n",
      "    text_string = reduce(lambda x, y: x + ' ' + y.strip('\\t'), text_list)\n",
      "    snippets.close()\n",
      "    return text_string\n",
      "\n",
      "\n",
      "#read file as unicode\n",
      "def loadCorpusAsUnicodeString(path):\n",
      "    rawtext = codecs.open(path,'r','utf-8').read()\n",
      "    return rawtext\n",
      "\n",
      "#tokenize into sentences\n",
      "def tokenizeCorpusFromTuples(title_content_tuples):\n",
      "    #tokenize into sentences\n",
      "    tokenizer = RegexpTokenizer('\\w+\\'\\w*|\\S+|[\\.!?,;:&-]') \n",
      "    '''\n",
      "    \\w+          strings of text, words\n",
      "    |            OR\n",
      "    [\\.!?,;:&-]  leave in meaningful punctuation\n",
      "    '''\n",
      "    #deal with title + contents tuples\n",
      "    docs = []\n",
      "    for i in title_content_tuples:\n",
      "        string = str(i[0]) + ' ' + str(i[1])\n",
      "        docs = docs + string\n",
      "    \n",
      "    #tokenize into words, removing punctuation\n",
      "    docs = docs.encode('iso-8859-1', 'ignore')\n",
      "    tokens = tokenizer.tokenize(docs)\n",
      "    #remove periods at end of strings, double commas, and weird double commas for more accurate FreqDist\n",
      "    tokens_clean = [t.rstrip('.').strip('\\'\\'').lstrip('``') for t in tokens]\n",
      "    return tokens_clean\n",
      "\n",
      "#tokenize into sentences\n",
      "def tokenizeCorpus(string):\n",
      "    #tokenize into sentences\n",
      "    tokenizer = RegexpTokenizer('\\w+\\'\\w*|\\S+|[\\.!?,;:&-]') \n",
      "    '''\n",
      "    \\w+          strings of text, words\n",
      "    |            OR\n",
      "    [\\.!?,;:&-]  leave in meaningful punctuation\n",
      "    '''\n",
      "    #tokenize into words, removing punctuation\n",
      "    string = string.encode('iso-8859-1', 'ignore') \n",
      "    tokens = tokenizer.tokenize(string)\n",
      "    #remove periods at end of strings, double commas, and weird double commas for more accurate FreqDist\n",
      "    tokens_clean = [t.rstrip('.').strip('\\'\\'').strip(')').strip('(').lstrip('``') for t in tokens]\n",
      "    return tokens_clean\n",
      "\n",
      "\n",
      "def htmlToTuples(filelist):\n",
      "    articles = {}\n",
      "    for f in filelist:\n",
      "        html = loadCorpusAsString(f)\n",
      "        soup = BeautifulSoup(html)\n",
      "        if soup.metadata.attrs['title'] not in articles:\n",
      "            articles[soup.metadata.attrs['title']] = soup.body.contents.pop()\n",
      "    return list(articles.iteritems())\n",
      "\n",
      "\n",
      "#read in film review corpus\n",
      "def getFreqBigramsfromFile(path):\n",
      "    corpus_string = loadCorpusAsString(path)\n",
      "    tokenized_corpus = tokenizeCorpus(corpus_string)\n",
      "    fd = freqBigramsFromTokens(tokenized_corpus)\n",
      "    return fd\n",
      "\n",
      "#read in film review corpus\n",
      "def getFreqBigramsfromCollection(file_list):\n",
      "    corpus_string = concatFilesUnicode(file_list)\n",
      "    tokenized_corpus = tokenizeCorpus(corpus_string)\n",
      "    fd = freqBigramsFromTokens(tokenized_corpus)\n",
      "    return fd\n",
      "\n",
      "#read in film review corpus\n",
      "def getFreqBigramsfromHTMLfiles(file_list):\n",
      "    title_content_tuples = htmlToTuples(file_list)\n",
      "    tokenized_corpus = tokenizeCorpusFromTuples(title_content_tuples)\n",
      "    fd = freqBigramsFromTokens(tokenized_corpus)\n",
      "    return fd\n",
      "\n",
      "#run functions separately on documents in corpus (can I pass functions as args to functions?)\n",
      "def runFunctionsOnFiles(file_list, func):\n",
      "    corpus = [loadCorpusAsString(f) for f in file_list]\n",
      "    for doc in corpus:\n",
      "        func(doc)\n",
      "        \n",
      "#from list of files, return concatenated string of all their contents        \n",
      "def concatFilesUnicode(file_list):\n",
      "    #load each file as string into list\n",
      "    corpus_list = [loadCorpusAsUnicodeString(f) for f in file_list]\n",
      "    #concatenate strings into single string\n",
      "    c = ''\n",
      "    for s in corpus_list:\n",
      "        c = c + s\n",
      "    #return string of entire corpus\n",
      "    return c\n",
      "\n",
      "def concatFiles(file_list):\n",
      "    #load each file as string into list\n",
      "    corpus_list = [loadCorpusAsString(f) for f in file_list]\n",
      "    #concatenate strings\n",
      "    c = ''\n",
      "    for s in corpus_list:\n",
      "        c = c + s\n",
      "    #return string of entire corpus\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to get string from file\n",
      "def loadJsonCorpusAsString(path):\n",
      "    snippets = open(path, \"r\")\n",
      "    text_list = snippets.readlines()\n",
      "    text_string = reduce(lambda x, y: x + ' ' + y.strip('\\t'), text_list)\n",
      "    snippets.close()\n",
      "    return text_string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 1. Read Files from Directory into List"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get list of files\n",
      "def readFilestoList(path):\n",
      "    filelist = []\n",
      "    for file in glob.glob(os.path.join(path, '*.json')):\n",
      "        filelist.append(file)\n",
      "    return filelist\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def jsonToTuples(filelist):\n",
      "    articles = []\n",
      "    for filepath in filelist:\n",
      "        with open(filepath, 'r') as f:\n",
      "            j = json.load(f)\n",
      "            if 'objects' in j.keys():\n",
      "                if 'title' in j['objects'][0]:\n",
      "                    articles.append((j['objects'][0]['title'], j['objects'][0]['text']))\n",
      "    return articles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "voterID = readFilestoList('data/voterID_clean/')  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "votID = jsonToTuples(voterID)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 2. Read Titles and Text into Tuple List"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 3. Put Titles, Content into Dataframe & Tokenize Both"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create df for titles & content\n",
      "#tokenize titles and content\n",
      "def tokenizeTitlesContentToDf(tuples):\n",
      "    articles = []\n",
      "    titles = []\n",
      "    for i in tuples:\n",
      "        articles.append(i[1])\n",
      "        titles.append(i[0])\n",
      "\n",
      "    data = {'article': articles, 'title': titles}\n",
      "    df = DataFrame(data)\n",
      "    df['tokenized_article'] = df['article'].map(nltk.word_tokenize)\n",
      "    df['tokenized_title'] = df['title'].map(nltk.word_tokenize)\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# health_it_df = tokenizeTitlesContentToDf(title_content_health_it)\n",
      "df_votID = tokenizeTitlesContentToDf(votID)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID['spanish_check'] = df_votID['tokenized_article'].map(nltk.FreqDist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng = df_votID[df_votID['spanish_check'].apply(lambda r: 'los' in r)==False]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(df_votID_eng)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(df_votID_eng[df_votID_eng['spanish_check'].apply(lambda r: 'los' in r)==True])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#functions for tokenizing sentences\n",
      "def listToTokens(sentenceList):\n",
      "    tokenized_sents = [nltk.word_tokenize(sent.strip('(').strip(')')) for sent in sentenceList]\n",
      "    return tokenized_sents\n",
      "\n",
      "def tagTokens(tokenList):\n",
      "    tagged_sents = [nltk.pos_tag(sent) for sent in tokenList]\n",
      "    return tagged_sents\n",
      "\n",
      "def tagTitle(title):\n",
      "    tagged_title = [nltk.pos_tag(title)]\n",
      "    return tagged_title\n",
      "\n",
      "\n",
      "#functions for adding columns to df of sentence tokenized, word tokenized, and pos tagged text\n",
      "def docsIntoSents(columnname, df):\n",
      "    df['sent_tokenized'] = df[columnname].map(nltk.sent_tokenize)\n",
      "    return df\n",
      "    \n",
      "def sentsIntoTokens(columnname, df):\n",
      "    df['word_tokenized'] = df[columnname].map(listToTokens)\n",
      "    return df\n",
      "    \n",
      "def tokensPOStag(columnname, df, option='article'):\n",
      "    if option=='article':\n",
      "        columntitle = 'pos_tagged_%s' %columnname\n",
      "        df[columntitle] = df[columnname].map(tagTokens)\n",
      "        return df\n",
      "    else:\n",
      "        columntitle = 'pos_tagged_%s' %columnname\n",
      "        df[columntitle] = df[columnname].map(tagTitle)\n",
      "        return df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng = docsIntoSents('article', df_votID_eng)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng = sentsIntoTokens('sent_tokenized', df_votID_eng)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng = tokensPOStag('word_tokenized', df_votID_eng)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng = tokensPOStag('tokenized_title', df_votID_eng, option='title')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#concatenate all rows in two of the columns of a dataframe\n",
      "def createCorpusMultiColumns(df, columnName1, columnname2):\n",
      "    corpus = []\n",
      "    for row in df[columnName1]:\n",
      "        corpus.extend(row)\n",
      "    for row in df[columnname2]:\n",
      "        corpus.extend(row)\n",
      "    return corpus\n",
      "\n",
      "def createCorpusSingleColumn(df, columnName1):\n",
      "    corpus = []\n",
      "    for row in df[columnName1]:\n",
      "        corpus.extend(row)\n",
      "    return corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 4. Concatenate Titles & Text into \"Corpus\" for Each Directory"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# votID_corpus = createCorpusSingleColumn(df_votID_2, 'word_tokenized')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Pattern-Based Collocations (Noun- & Verb-Phrase Chunking)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#look for important noun phrase patterns, either nouns preceded by Det+Adj or simply compound nouns\n",
      "def parseImportantNps():\n",
      "    grammar = r\"\"\"\n",
      "    N-N: {<DET|CD.*>?<J*|N.*>+<N.*>} # chunk DET/Cardinal w/optional ADJ or N with proper noun\n",
      "           {<NNP.*>+<NNP.*>}             # chunk solo proper nouns only\n",
      "    \"\"\"\n",
      "    cp = nltk.RegexpParser(grammar)\n",
      "    return cp, 'N-N'\n",
      "\n",
      "\n",
      "#function to chunk \n",
      "def chunk(tagged, func=parseImportantNps):\n",
      "    chunks = []\n",
      "    leaves = []\n",
      "    cp, cn = func()\n",
      "    for i in tagged:\n",
      "        tree = cp.parse(i)\n",
      "        for subtree in tree.subtrees():\n",
      "            if subtree.node == cn:\n",
      "                leaflist = [leaf[0] for leaf in subtree.leaves()]\n",
      "                chunks.append(subtree.leaves())\n",
      "                leaves.append(' '.join(leaflist))\n",
      "    return leaves\n",
      "        \n",
      "#show the proper nouns as strings without their tags\n",
      "def getChunksAsStrings(chunk_list):\n",
      "    phrases = []\n",
      "    for i in chunk_list:\n",
      "        phrase = ''\n",
      "        for j in i:\n",
      "            phrase = phrase + j[0] + ' '\n",
      "        phrases.append(phrase)\n",
      "    return phrases    \n",
      "\n",
      "\n",
      "def freqDistChunks(preprocessed, func=1):\n",
      "    chunks = chunk(preprocessed, func=func)\n",
      "    chunks_only = getChunksAsStrings(chunks)\n",
      "    chks = FreqDist(chunks_only)  \n",
      "    if func==1:\n",
      "        n_list = []\n",
      "        print 'Important Noun Phrases'\n",
      "        for n in chks.items():\n",
      "            if n[1] > 5:\n",
      "                print '%-10s' % n[0], '%-10s' % n[1]\n",
      "                n_list.append(['%-10s' % n[0], '%-10s' % n[1]])\n",
      "        return n_list\n",
      "    elif func==2:\n",
      "        p_list = []\n",
      "        print 'Important Predicate Phrases'\n",
      "        for p in chks.items():\n",
      "            if p[1] > 5:\n",
      "                print '%-10s' % p[0], '%-10s' % p[1]\n",
      "                p_list.append(['%-10s' % p[0], '%-10s' % p[1]])\n",
      "        return p_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# function to add new column with tagged tokens (whether from article or title) to df\n",
      "def tokensPOStag(columnname, df, option='article'):\n",
      "    if option=='article':\n",
      "        columntitle = 'pos_tagged_%s' %columnname\n",
      "        df[columntitle] = df[columnname].map(tagTokens)\n",
      "        return df\n",
      "    else:\n",
      "        columntitle = 'pos_tagged_%s' %columnname\n",
      "        df[columntitle] = df[columnname].map(tagTitle)\n",
      "        return df\n",
      "\n",
      "def chunkColumn(columnname, df):\n",
      "    columntitle = 'chunked_%s' %columnname\n",
      "    df[columntitle] = df[columnname].map(chunk)\n",
      "    return df\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#blank articles\n",
      "df_votID_eng[df_votID_eng.pos_tagged_word_tokenized.apply(lambda r: len(r[0]))==0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng['sent_tokenized'][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng = chunkColumn('pos_tagged_word_tokenized', df_votID_eng)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng['article_chunks'] = df_votID_eng.pos_tagged_word_tokenized.apply(chunk)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng['article_chunks'][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_votID_eng.title.count(), len(df_votID_eng.title.unique())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_chunks = []\n",
      "for chunk in df_votID_eng['article_chunks']:\n",
      "    corpus_chunks.extend(chunk)\n",
      "     \n",
      "chunk_fd = nltk.FreqDist(corpus_chunks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freqchunks = chunk_fd.keys()[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunk_fd.items()[10], chunk_fd.items()[100], chunk_fd.items()[1000], chunk_fd.items()[10000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunk_fd.items()[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_sents = createCorpusSingleColumn(df_votID_eng, 'sent_tokenized')\n",
      "all_sents[:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freqchunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imp_sents_dict = {} # get sentences with a particular word in them\n",
      "\n",
      "for fc in freqchunks:\n",
      "    imp_sents = []\n",
      "    imp_sents_dict[fc] = imp_sents\n",
      "    for sent in all_sents:\n",
      "        if sent.find(fc) != -1:\n",
      "            imp_sents.append(sent)\n",
      "            \n",
      "for l in imp_sents_dict.values():\n",
      "    print len(l)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "total_sents = []\n",
      "\n",
      "for article in df_votID_eng['sent_tokenized']:\n",
      "    for s in article:\n",
      "        total_sents.append(s)\n",
      "    \n",
      "len(total_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_sents = []\n",
      "for l in imp_sents_dict.values():\n",
      "    for sent in l:\n",
      "        all_sents.extend(l)\n",
      "\n",
      "sents_of_interest = set(all_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "sent_dict = {}\n",
      "\n",
      "for sent in sents_of_interest:\n",
      "    chunks_contained = {}\n",
      "    fcs = []\n",
      "    score = 0\n",
      "    \n",
      "    if sent not in sent_dict:\n",
      "        sent_dict[sent] = chunks_contained\n",
      "        chunks_contained['fcs'] = fcs\n",
      "        \n",
      "        \n",
      "        for fc in freqchunks:\n",
      "            if sent.find(fc) != -1:\n",
      "                fcs.append(fc)\n",
      "                score = score + (len(fc.split())**2)\n",
      "        chunks_contained['score'] = int(score)\n",
      "\n",
      "sent_dict.items()[-10:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summ_sents = list(sent_dict.items())\n",
      "summ_sents.sort(key=lambda x: x[1]['score']) \n",
      "summ_sents[-10:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def integrate_ngrams(new):\n",
      "    \"Takes a list of integrates lists of different sized ngrams\"\n",
      "    out = []\n",
      "    orig_reps = []\n",
      "    \n",
      "    for t in new:\n",
      "        if type(t)!='str':\n",
      "            rep = \" \".join(t)\n",
      "        else:\n",
      "            rep = t\n",
      "        \n",
      "        if not any([rep in o_rep for o_rep in orig_reps]):\n",
      "            out.append(t)\n",
      "    \n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in summ_sents:\n",
      "    if len(i[1]['fcs']) > 3:\n",
      "        print i[1]['fcs']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ord_freqchunks = sorted(freqchunks, key=len)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ord_freqchunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = ['an apple', 'apple', 'banana and an apple']\n",
      "integrate_ngrams(a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get FD of chunks (or other method for getting keyphrases) across corpus\n",
      "#pull out all sentences with those keyphrases in them from corpus into mini corpus\n",
      "#look at all sentences\n",
      "#chunk for all verbs and look for phrases within these sentences\n",
      "#develop a matrix (docs & )\n",
      "#what are all the things that are associated \n",
      "#reduce chunks to get summary (get to tag cloud -- THIS IS OUR BASELINE)\n",
      "\n",
      "\n",
      "#then try on additional hashtag set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___________________"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Second Approach: Collocations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Statistics-based Bigrams & Trigrams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part II. Collocations <br>\n",
      "* Statistics-based Collocations (PMI, Chi-squared, etc)\n",
      "* Syntactic-Pattern-Based Collocations (as in Justeson & Katz)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, I took an approach of comparing similar but different bigram collocation finder algorithms to see if less restrictive or more restrictive formulas yielded better results. Less generally means that I'll accept those that occur more infrequently AND in a larger \"window size\", so they can span other text. More restrictive is the opposite."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create tokens from string\n",
      "def getTokens(string):\n",
      "    sentences = nltk.sent_tokenize(string)\n",
      "    tokenized_sents = [nltk.word_tokenize(sent.strip('(').strip(')')) for sent in sentences]\n",
      "    tokens = [unicode(w,\"latin-1\") for i in tokenized_sents for w in i]\n",
      "    return tokens\n",
      "\n",
      "#1\n",
      "#bigrams that appear at least 5 times within a window size of 6 words, excluding any bigram \n",
      "#with a token of fewer than 3 chars; return the 20 best according to the PMI measure\n",
      "def getBiCollocations(word_list):\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    #look for bigrams near each other in a window of 10\n",
      "    finder = BigramCollocationFinder.from_words(word_list, window_size=10)\n",
      "    finder.apply_freq_filter(3)\n",
      "    ignored_words = nltk.corpus.stopwords.words('english')\n",
      "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
      "    return finder.nbest(bigram_measures.pmi, 50)  \n",
      "\n",
      "#2\n",
      "#bigrams that appear at least 10 times within a window size of 4 words, excluding any bigram \n",
      "#with a token of fewer than 3 chars; return the 20 best according to the PMI measure\n",
      "def getBiCollocations2(word_list):\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    #look for bigrams near each other in a window of 4\n",
      "    finder = BigramCollocationFinder.from_words(word_list, window_size=4)\n",
      "    finder.apply_freq_filter(5)\n",
      "    ignored_words = nltk.corpus.stopwords.words('english')\n",
      "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
      "    return finder.nbest(bigram_measures.pmi, 50)  \n",
      "\n",
      "#3\n",
      "#find trigrams that appear at least 3 times, excluding any bigram with a token of fewer than 3 \n",
      "#chars; return the 20 best according to the PMI measure\n",
      "def getTriCollocations(word_list):\n",
      "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "    finder = TrigramCollocationFinder.from_words(word_list)\n",
      "    finder.apply_freq_filter(2)\n",
      "    ignored_words = nltk.corpus.stopwords.words('english')\n",
      "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
      "    return finder.nbest(trigram_measures.pmi, 20)  \n",
      "      \n",
      "\n",
      "#prettifies the display of bigram and trigram collocation functions\n",
      "def findCollocsfromTokens(tokens, func=1):\n",
      "    if func == 1:\n",
      "        bi = getBiCollocations(tokens)\n",
      "        print 'Important Bigram Collocations (Loose Measures)'\n",
      "        print \n",
      "        for e in bi:\n",
      "            print '%-10s' % e[0], '%-10s' % e[1]\n",
      "    elif func == 2:\n",
      "        bi2 = getBiCollocations2(tokens)\n",
      "        print 'Important Bigram Collocations (Restrictive Measures)'\n",
      "        print\n",
      "        for f in bi2:\n",
      "            print '%-10s' % f[0], '%-10s' % f[1]\n",
      "    elif func == 3: \n",
      "        tri = getTriCollocations(tokens)\n",
      "        print 'Important Trigram Collocations'\n",
      "        print\n",
      "        for j in tri:\n",
      "            print '%-10s' % j[0], '%-10s' % j[1], '%-10s' % j[2]\n",
      "    else: \n",
      "        print 'func takes either 1, 2, or 3 as values'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "______"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}