{
 "metadata": {
  "gist_id": "f00815cbe158a89cf35e",
  "name": "",
  "signature": "sha256:73f471fa6820ed39ac575ed8e931edf725fa395a7fe696543a7e11e78b64dec7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, string\n",
      "from nltk.corpus import stopwords, brown\n",
      "from nltk.book import *\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.collocations import *\n",
      "import codecs\n",
      "import os\n",
      "import glob\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import wordnet as wn\n",
      "from pandas import DataFrame\n",
      "import pandas as pd\n",
      "wnlemmatizer = nltk.WordNetLemmatizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "*** Introductory Examples for the NLTK Book ***\n",
        "Loading text1, ..., text9 and sent1, ..., sent9\n",
        "Type the name of the text or sentence to view it.\n",
        "Type: 'texts()' or 'sents()' to list the materials.\n",
        "text1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Moby Dick by Herman Melville 1851\n",
        "text2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Sense and Sensibility by Jane Austen 1811\n",
        "text3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Book of Genesis\n",
        "text4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Inaugural Address Corpus\n",
        "text5:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Chat Corpus\n",
        "text6: Monty Python and the Holy Grail\n",
        "text7:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Wall Street Journal\n",
        "text8: Personals Corpus\n",
        "text9:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Man Who Was Thursday by G . K . Chesterton 1908\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part I.\n",
      "Showing Frequent Terms <br>\n",
      "* Frequent Unigrams (with or without stemming, stopwords, and other normalization)\n",
      "* Frequent Bigram frequencies (with or without stemming, stopwords, and other normalization)\n",
      "* Other variations on frequent n-grams"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "First Approach: Simple Frequency Distribution of Single Terms, aka Unigrams"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Baseline Algorithm: Frequency Distribution on Unigrams"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#freq dist function from set of corpus tokens\n",
      "\n",
      "def freqBigramsFromTokens(tokenList):\n",
      "    #modify this list based on corpus\n",
      "    punc = ['.',',', ';', '--', '(', ')', ':', '``', '\\'\\'', '?']\n",
      "    #additional words that should be treated like stopwords after initial assessment of performance\n",
      "    mod_num = ['one','two','three','four','five','first', 'second','third','fourth','fifth', \\\n",
      "               'could','should','would','might','must','will','can']\n",
      "    #concatenate stopwords, punctuation, + special modals/numbers/cardinals list\n",
      "    sw_p = nltk.corpus.stopwords.words('english') + punc + mod_num\n",
      "    wnl_lemmas = [wnlemmatizer.lemmatize(word.lower()) for word in tokenList if word.lower() not in sw_p]\n",
      "    fd = nltk.FreqDist(bigrams(wnl_lemmas))\n",
      "    top_n = fd.items()[:50]\n",
      "    top_n_vals = fd.values()[:50]\n",
      "    i = 0\n",
      "    print '%-20s' % 'WORD PAIR ', '%-20s' % 'BIGRAM COUNT'\n",
      "    for b in top_n:\n",
      "        print '%-12s' % b[0][0], '%-12s' % b[0][1], '%-16d' % top_n_vals[i]\n",
      "        i=i+1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#showing stopwords\n",
      "nltk.corpus.stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "['i',\n",
        " 'me',\n",
        " 'my',\n",
        " 'myself',\n",
        " 'we',\n",
        " 'our',\n",
        " 'ours',\n",
        " 'ourselves',\n",
        " 'you',\n",
        " 'your',\n",
        " 'yours',\n",
        " 'yourself',\n",
        " 'yourselves',\n",
        " 'he',\n",
        " 'him',\n",
        " 'his',\n",
        " 'himself',\n",
        " 'she',\n",
        " 'her',\n",
        " 'hers',\n",
        " 'herself',\n",
        " 'it',\n",
        " 'its',\n",
        " 'itself',\n",
        " 'they',\n",
        " 'them',\n",
        " 'their',\n",
        " 'theirs',\n",
        " 'themselves',\n",
        " 'what',\n",
        " 'which',\n",
        " 'who',\n",
        " 'whom',\n",
        " 'this',\n",
        " 'that',\n",
        " 'these',\n",
        " 'those',\n",
        " 'am',\n",
        " 'is',\n",
        " 'are',\n",
        " 'was',\n",
        " 'were',\n",
        " 'be',\n",
        " 'been',\n",
        " 'being',\n",
        " 'have',\n",
        " 'has',\n",
        " 'had',\n",
        " 'having',\n",
        " 'do',\n",
        " 'does',\n",
        " 'did',\n",
        " 'doing',\n",
        " 'a',\n",
        " 'an',\n",
        " 'the',\n",
        " 'and',\n",
        " 'but',\n",
        " 'if',\n",
        " 'or',\n",
        " 'because',\n",
        " 'as',\n",
        " 'until',\n",
        " 'while',\n",
        " 'of',\n",
        " 'at',\n",
        " 'by',\n",
        " 'for',\n",
        " 'with',\n",
        " 'about',\n",
        " 'against',\n",
        " 'between',\n",
        " 'into',\n",
        " 'through',\n",
        " 'during',\n",
        " 'before',\n",
        " 'after',\n",
        " 'above',\n",
        " 'below',\n",
        " 'to',\n",
        " 'from',\n",
        " 'up',\n",
        " 'down',\n",
        " 'in',\n",
        " 'out',\n",
        " 'on',\n",
        " 'off',\n",
        " 'over',\n",
        " 'under',\n",
        " 'again',\n",
        " 'further',\n",
        " 'then',\n",
        " 'once',\n",
        " 'here',\n",
        " 'there',\n",
        " 'when',\n",
        " 'where',\n",
        " 'why',\n",
        " 'how',\n",
        " 'all',\n",
        " 'any',\n",
        " 'both',\n",
        " 'each',\n",
        " 'few',\n",
        " 'more',\n",
        " 'most',\n",
        " 'other',\n",
        " 'some',\n",
        " 'such',\n",
        " 'no',\n",
        " 'nor',\n",
        " 'not',\n",
        " 'only',\n",
        " 'own',\n",
        " 'same',\n",
        " 'so',\n",
        " 'than',\n",
        " 'too',\n",
        " 'very',\n",
        " 's',\n",
        " 't',\n",
        " 'can',\n",
        " 'will',\n",
        " 'just',\n",
        " 'don',\n",
        " 'should',\n",
        " 'now']"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Reminder of What Bigrams Are"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#concept of bigrams\n",
      "sent = ['i', 'am', 'sitting', 'at', 'a','coffee','shop','with','andy', 'and','i','am','working']\n",
      "bigrams(sent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[('i', 'am'),\n",
        " ('am', 'sitting'),\n",
        " ('sitting', 'at'),\n",
        " ('at', 'a'),\n",
        " ('a', 'coffee'),\n",
        " ('coffee', 'shop'),\n",
        " ('shop', 'with'),\n",
        " ('with', 'andy'),\n",
        " ('andy', 'and'),\n",
        " ('and', 'i'),\n",
        " ('i', 'am'),\n",
        " ('am', 'working')]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Functions Used for Reading Files from Directory, Tokenizing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#function to get string from file\n",
      "def loadCorpusAsString(path):\n",
      "    snippets = open(path, \"r\")\n",
      "    text_list = snippets.readlines()\n",
      "    snippets.close()\n",
      "    text_string = reduce(lambda x, y: x + ' ' + y.strip('\\t'), text_list)\n",
      "    return text_string\n",
      "\n",
      "#read file as unicode\n",
      "def loadCorpusAsUnicodeString(path):\n",
      "    rawtext = codecs.open(path,'r','utf-8').read()\n",
      "    return rawtext\n",
      "\n",
      "\n",
      "#tokenize into sentences\n",
      "def tokenizeCorpusFromTuples(title_content_tuples):\n",
      "    #tokenize into sentences\n",
      "    tokenizer = RegexpTokenizer('\\w+\\'\\w*|\\S+|[\\.!?,;:&-]') \n",
      "    '''\n",
      "    \\w+          strings of text, words\n",
      "    |            OR\n",
      "    [\\.!?,;:&-]  leave in meaningful punctuation\n",
      "    '''\n",
      "    #deal with title + contents tuples\n",
      "    docs = []\n",
      "    for i in title_content_tuples:\n",
      "        string = str(i[0]) + ' ' + str(i[1])\n",
      "        docs = docs + string\n",
      "    \n",
      "    #tokenize into words, removing punctuation\n",
      "    docs = docs.encode('iso-8859-1', 'ignore')\n",
      "    tokens = tokenizer.tokenize(docs)\n",
      "    #remove periods at end of strings, double commas, and weird double commas for more accurate FreqDist\n",
      "    tokens_clean = [t.rstrip('.').strip('\\'\\'').lstrip('``') for t in tokens]\n",
      "    return tokens_clean\n",
      "\n",
      "#tokenize into sentences\n",
      "def tokenizeCorpus(string):\n",
      "    #tokenize into sentences\n",
      "    tokenizer = RegexpTokenizer('\\w+\\'\\w*|\\S+|[\\.!?,;:&-]') \n",
      "    '''\n",
      "    \\w+          strings of text, words\n",
      "    |            OR\n",
      "    [\\.!?,;:&-]  leave in meaningful punctuation\n",
      "    '''\n",
      "    #tokenize into words, removing punctuation\n",
      "    string = string.encode('iso-8859-1', 'ignore') \n",
      "    tokens = tokenizer.tokenize(string)\n",
      "    #remove periods at end of strings, double commas, and weird double commas for more accurate FreqDist\n",
      "    tokens_clean = [t.rstrip('.').strip('\\'\\'').strip(')').strip('(').lstrip('``') for t in tokens]\n",
      "    return tokens_clean\n",
      "\n",
      "\n",
      "def htmlToTuples(filelist):\n",
      "    articles = {}\n",
      "    for f in filelist:\n",
      "        html = loadCorpusAsString(f)\n",
      "        soup = BeautifulSoup(html)\n",
      "        if soup.metadata.attrs['title'] not in articles:\n",
      "            articles[soup.metadata.attrs['title']] = soup.body.contents.pop()\n",
      "    return list(articles.iteritems())\n",
      "\n",
      "\n",
      "#read in film review corpus\n",
      "def getFreqBigramsfromFile(path):\n",
      "    corpus_string = loadCorpusAsString(path)\n",
      "    tokenized_corpus = tokenizeCorpus(corpus_string)\n",
      "    fd = freqBigramsFromTokens(tokenized_corpus)\n",
      "    return fd\n",
      "\n",
      "#read in film review corpus\n",
      "def getFreqBigramsfromCollection(file_list):\n",
      "    corpus_string = concatFilesUnicode(file_list)\n",
      "    tokenized_corpus = tokenizeCorpus(corpus_string)\n",
      "    fd = freqBigramsFromTokens(tokenized_corpus)\n",
      "    return fd\n",
      "\n",
      "#read in film review corpus\n",
      "def getFreqBigramsfromHTMLfiles(file_list):\n",
      "    title_content_tuples = htmlToTuples(file_list)\n",
      "    tokenized_corpus = tokenizeCorpusFromTuples(title_content_tuples)\n",
      "    fd = freqBigramsFromTokens(tokenized_corpus)\n",
      "    return fd\n",
      "\n",
      "#run functions separately on documents in corpus (can I pass functions as args to functions?)\n",
      "def runFunctionsOnFiles(file_list, func):\n",
      "    corpus = [loadCorpusAsString(f) for f in file_list]\n",
      "    for doc in corpus:\n",
      "        func(doc)\n",
      "        \n",
      "#from list of files, return concatenated string of all their contents        \n",
      "def concatFilesUnicode(file_list):\n",
      "    #load each file as string into list\n",
      "    corpus_list = [loadCorpusAsUnicodeString(f) for f in file_list]\n",
      "    #concatenate strings into single string\n",
      "    c = ''\n",
      "    for s in corpus_list:\n",
      "        c = c + s\n",
      "    #return string of entire corpus\n",
      "    return c\n",
      "\n",
      "def concatFiles(file_list):\n",
      "    #load each file as string into list\n",
      "    corpus_list = [loadCorpusAsString(f) for f in file_list]\n",
      "    #concatenate strings\n",
      "    c = ''\n",
      "    for s in corpus_list:\n",
      "        c = c + s\n",
      "    #return string of entire corpus\n",
      "    return c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 1. Read Files from Directory into List"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get list of files in health_it and health_law_policy folder\n",
      "def readFilestoList(path):\n",
      "    filelist = []\n",
      "    for file in glob.glob(os.path.join(path, '*.txt')):\n",
      "        filelist.append(file)\n",
      "    return filelist\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "three_d_printing = readFilestoList('3d_printing/')  \n",
      "health_law_policy = readFilestoList('Health-Law-Policy/')        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 2. Read Titles and Text into Tuple List"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get titles and content into tuples\n",
      "# title_content_health_it = htmlToTuples(health_it)\n",
      "title_content_health_law_policy = htmlToTuples(health_law_policy)\n",
      "title_content_3d_print = htmlToTuples(three_d_printing)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 3. Put Titles, Content into Dataframe & Tokenize Both"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create df for titles & content\n",
      "#tokenize titles and content\n",
      "def tokenizeTitlesContentToDf(tuples):\n",
      "    articles = []\n",
      "    titles = []\n",
      "    for i in tuples:\n",
      "        articles.append(i[0])\n",
      "        titles.append(i[1])\n",
      "\n",
      "    data = {'article': articles, 'title': titles}\n",
      "    df = DataFrame(data)\n",
      "    df['tokenized_article'] = df['article'].map(nltk.word_tokenize)\n",
      "    df['tokenized_title'] = df['title'].map(nltk.word_tokenize)\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# health_it_df = tokenizeTitlesContentToDf(title_content_health_it)\n",
      "health_lp_df = tokenizeTitlesContentToDf(title_content_health_law_policy)\n",
      "three_d_df = tokenizeTitlesContentToDf(title_content_3d_print)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "three_d_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>article</th>\n",
        "      <th>title</th>\n",
        "      <th>tokenized_article</th>\n",
        "      <th>tokenized_title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "Empty DataFrame\n",
        "Columns: [article, title, tokenized_article, tokenized_title]\n",
        "Index: []"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "health_it_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>article</th>\n",
        "      <th>title</th>\n",
        "      <th>tokenized_article</th>\n",
        "      <th>tokenized_title</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Upping in ante in Pittsburgh: the health infor...</td>\n",
        "      <td> \n",
        " \n",
        " A few weeks ago, UPMC announced an agreeme...</td>\n",
        "      <td> [Upping, in, ante, in, Pittsburgh, :, the, hea...</td>\n",
        "      <td> [A, few, weeks, ago, ,, UPMC, announced, an, a...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> Health 2.0: Adopting Health Information Techno...</td>\n",
        "      <td> \n",
        " \n",
        " Past Event\n",
        " Event Materials\n",
        " Summary\n",
        " With...</td>\n",
        "      <td> [Health, 2.0, :, Adopting, Health, Information...</td>\n",
        "      <td> [Past, Event, Event, Materials, Summary, With,...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>      Policy Implications of Education Informatics</td>\n",
        "      <td> \n",
        " \n",
        " This concluding article identifies the num...</td>\n",
        "      <td> [Policy, Implications, of, Education, Informat...</td>\n",
        "      <td> [This, concluding, article, identifies, the, n...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> Bending the Curve through Health Reform Implem...</td>\n",
        "      <td> \n",
        " \n",
        " \n",
        " EXECUTIVE SUMMARY:\u00a0 In September 2009,\u00a0w...</td>\n",
        "      <td> [Bending, the, Curve, through, Health, Reform,...</td>\n",
        "      <td> [EXECUTIVE, SUMMARY, :, In, September, 2009, ,...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  Third Annual Sentinel Initiative Public Workshop</td>\n",
        "      <td> \n",
        " \n",
        " \n",
        " \n",
        " Event Information\n",
        " \n",
        " January 12, 20118...</td>\n",
        "      <td> [Third, Annual, Sentinel, Initiative, Public, ...</td>\n",
        "      <td> [Event, Information, January, 12, ,, 20118:30,...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "                                             article  \\\n",
        "0  Upping in ante in Pittsburgh: the health infor...   \n",
        "1  Health 2.0: Adopting Health Information Techno...   \n",
        "2       Policy Implications of Education Informatics   \n",
        "3  Bending the Curve through Health Reform Implem...   \n",
        "4   Third Annual Sentinel Initiative Public Workshop   \n",
        "\n",
        "                                               title  \\\n",
        "0  \n",
        " \n",
        " A few weeks ago, UPMC announced an agreeme...   \n",
        "1  \n",
        " \n",
        " Past Event\n",
        " Event Materials\n",
        " Summary\n",
        " With...   \n",
        "2  \n",
        " \n",
        " This concluding article identifies the num...   \n",
        "3  \n",
        " \n",
        " \n",
        " EXECUTIVE SUMMARY:\u00a0 In September 2009,\u00a0w...   \n",
        "4  \n",
        " \n",
        " \n",
        " \n",
        " Event Information\n",
        " \n",
        " January 12, 20118...   \n",
        "\n",
        "                                   tokenized_article  \\\n",
        "0  [Upping, in, ante, in, Pittsburgh, :, the, hea...   \n",
        "1  [Health, 2.0, :, Adopting, Health, Information...   \n",
        "2  [Policy, Implications, of, Education, Informat...   \n",
        "3  [Bending, the, Curve, through, Health, Reform,...   \n",
        "4  [Third, Annual, Sentinel, Initiative, Public, ...   \n",
        "\n",
        "                                     tokenized_title  \n",
        "0  [A, few, weeks, ago, ,, UPMC, announced, an, a...  \n",
        "1  [Past, Event, Event, Materials, Summary, With,...  \n",
        "2  [This, concluding, article, identifies, the, n...  \n",
        "3  [EXECUTIVE, SUMMARY, :, In, September, 2009, ,...  \n",
        "4  [Event, Information, January, 12, ,, 20118:30,...  "
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get simple frequency distribution from df\n",
      "def createCorpus(df, columnName1, columnname2):\n",
      "    corpus = []\n",
      "    for row in df[columnName1]:\n",
      "        corpus.extend(row)\n",
      "    for row in df[columnname2]:\n",
      "        corpus.extend(row)\n",
      "    return corpus\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 4. Concatenate Titles & Text into \"Corpus\" for Each Directory (i.e. Health IT) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#frequency dist from health it and law and policy\n",
      "#health_it_corpus is one long list of all the tokens contained in all the health_it articles\n",
      "health_it_corpus = createCorpus(health_it_df, 'tokenized_article', 'tokenized_title')\n",
      "health_lp_corpus = createCorpus(health_lp_df, 'tokenized_article', 'tokenized_title')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 5. Run Frequency Bigrams Algorithm on Tokens"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freqBigramsFromTokens(health_it_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WORD PAIR            BIGRAM COUNT        \n",
        "rest         bill         1               \n",
        "right        direction    1               \n",
        "rule         regulation   1               \n",
        "local        hospital     2               \n",
        "encounter    different    1               \n",
        "dominated    physician    1               \n",
        "information  collected    2               \n",
        "take         medication   3               \n",
        "possible     select       1               \n",
        "commitment   opportunity  1               \n",
        "much         money        1               \n",
        "use          cover        1               \n",
        "enter        republican   1               \n",
        "mechanism    coordinating 1               \n",
        "'s           medical      1               \n",
        "company      \u201csticking    1               \n",
        "place        cover        1               \n",
        "\u201cold         days\u201d        1               \n",
        "say          program      1               \n",
        "percent      suffer       1               \n",
        "bypass       surgery      1               \n",
        "flexible     lead         1               \n",
        "regulated    market       1               \n",
        "wide         range        3               \n",
        "system       bring        1               \n",
        "set          appointment  1               \n",
        "evaporate    notion       1               \n",
        "data         publication  1               \n",
        "clinical     care         8               \n",
        "economic     sustainability 1               \n",
        "7,900        current      1               \n",
        "period       however      1               \n",
        "experience   greater      1               \n",
        "making       genetic      1               \n",
        "information  disease      2               \n",
        "woman        robert       1               \n",
        "iclosing     remark       1               \n",
        "paperless    young        1               \n",
        "surrounding  interoperability 1               \n",
        "political    agenda\u2014his   1               \n",
        "representative controlled   1               \n",
        "greenberggreenberg quinlan      1               \n",
        "upmc         hospital     1               \n",
        "implication  offer        1               \n",
        "congress     effectively  1               \n",
        "see          movie        1               \n",
        "official     office       1               \n",
        "deven        mcgraw       1               \n",
        "interpreted  outline      1               \n",
        "dioxide      produced     1               \n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freqBigramsFromTokens(health_lp_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WORD PAIR            BIGRAM COUNT        \n",
        "mandate      explores     1               \n",
        "rest         bill         1               \n",
        "identifying  gene         1               \n",
        "association  kaiser-permanente 1               \n",
        "employer     saved        1               \n",
        "commitment   opportunity  1               \n",
        "agencias     internacionales\u2013 1               \n",
        "younger      person       1               \n",
        "providers\u2014is nonexistent  1               \n",
        "habit        month        1               \n",
        "manner       time         1               \n",
        "luxury       emerging     1               \n",
        "peak         yet          1               \n",
        "cost         driver       1               \n",
        "gilbert      burnham      1               \n",
        "thereby      reducing     1               \n",
        "people       disability   1               \n",
        "source       \u00a9            3               \n",
        "blitz        promoting    1               \n",
        "people       represent    2               \n",
        "accessible   feature      1               \n",
        "cut          medicare\u2019s   1               \n",
        "representative controlled   1               \n",
        "surveyed     u.s.         1               \n",
        "mind         time         1               \n",
        "demand       supply       2               \n",
        "iezzoni      professor    1               \n",
        "lower-income american     1               \n",
        "use          monetary     1               \n",
        "pounds.the   united       1               \n",
        "comparative  youthfulness 1               \n",
        "program      produce\u2014and  1               \n",
        "identify     possible     1               \n",
        "protein      gene         3               \n",
        "insures      240,000      1               \n",
        "driver       home         1               \n",
        "actor        also         1               \n",
        "conflict     achieving    1               \n",
        "remark       -            1               \n",
        "billion      said         1               \n",
        "others       dwell        1               \n",
        "trying       precisely    1               \n",
        "make         grant        4               \n",
        "50           patient      1               \n",
        "way          patient      1               \n",
        "people       died         2               \n",
        "stimulus     contained    1               \n",
        "focus        united       1               \n",
        "good         health       1               \n",
        "university   southern     2               \n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "___________________"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Second Approach: Collocations"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Statistics-based Bigrams & Trigrams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part II. Collocations <br>\n",
      "* Statistics-based Collocations (PMI, Chi-squared, etc)\n",
      "* Syntactic-Pattern-Based Collocations (as in Justeson & Katz)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, I took an approach of comparing similar but different bigram collocation finder algorithms to see if less restrictive or more restrictive formulas yielded better results. Less generally means that I'll accept those that occur more infrequently AND in a larger \"window size\", so they can span other text. More restrictive is the opposite."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#create tokens from string\n",
      "def getTokens(string):\n",
      "    sentences = nltk.sent_tokenize(string)\n",
      "    tokenized_sents = [nltk.word_tokenize(sent.strip('(').strip(')')) for sent in sentences]\n",
      "    tokens = [unicode(w,\"latin-1\") for i in tokenized_sents for w in i]\n",
      "    return tokens\n",
      "\n",
      "#1\n",
      "#bigrams that appear at least 5 times within a window size of 6 words, excluding any bigram \n",
      "#with a token of fewer than 3 chars; return the 20 best according to the PMI measure\n",
      "def getBiCollocations(word_list):\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    #look for bigrams near each other in a window of 10\n",
      "    finder = BigramCollocationFinder.from_words(word_list, window_size=10)\n",
      "    finder.apply_freq_filter(3)\n",
      "    ignored_words = nltk.corpus.stopwords.words('english')\n",
      "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
      "    return finder.nbest(bigram_measures.pmi, 50)  \n",
      "\n",
      "#2\n",
      "#bigrams that appear at least 10 times within a window size of 4 words, excluding any bigram \n",
      "#with a token of fewer than 3 chars; return the 20 best according to the PMI measure\n",
      "def getBiCollocations2(word_list):\n",
      "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
      "    #look for bigrams near each other in a window of 4\n",
      "    finder = BigramCollocationFinder.from_words(word_list, window_size=4)\n",
      "    finder.apply_freq_filter(5)\n",
      "    ignored_words = nltk.corpus.stopwords.words('english')\n",
      "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
      "    return finder.nbest(bigram_measures.pmi, 50)  \n",
      "\n",
      "#3\n",
      "#find trigrams that appear at least 3 times, excluding any bigram with a token of fewer than 3 \n",
      "#chars; return the 20 best according to the PMI measure\n",
      "def getTriCollocations(word_list):\n",
      "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "    finder = TrigramCollocationFinder.from_words(word_list)\n",
      "    finder.apply_freq_filter(2)\n",
      "    ignored_words = nltk.corpus.stopwords.words('english')\n",
      "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
      "    return finder.nbest(trigram_measures.pmi, 20)  \n",
      "      \n",
      "\n",
      "#prettifies the display of bigram and trigram collocation functions\n",
      "def findCollocsfromTokens(tokens, func=1):\n",
      "    if func == 1:\n",
      "        bi = getBiCollocations(tokens)\n",
      "        print 'Important Bigram Collocations (Loose Measures)'\n",
      "        print \n",
      "        for e in bi:\n",
      "            print '%-10s' % e[0], '%-10s' % e[1]\n",
      "    elif func == 2:\n",
      "        bi2 = getBiCollocations2(tokens)\n",
      "        print 'Important Bigram Collocations (Restrictive Measures)'\n",
      "        print\n",
      "        for f in bi2:\n",
      "            print '%-10s' % f[0], '%-10s' % f[1]\n",
      "    elif func == 3: \n",
      "        tri = getTriCollocations(tokens)\n",
      "        print 'Important Trigram Collocations'\n",
      "        print\n",
      "        for j in tri:\n",
      "            print '%-10s' % j[0], '%-10s' % j[1], '%-10s' % j[2]\n",
      "    else: \n",
      "        print 'func takes either 1, 2, or 3 as values'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 1. Run Corpus (list of all tokens in corpus) Through Bigram Collocations (Loose Measures) Algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find top 50 frequently occurring pairs of words which occur AT LEAST 3 times in document and within 10 words of each other\n",
      "#ignore stopwords and words of 3 chars or fewer\n",
      "findCollocsfromTokens(health_it_corpus, func=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Important Bigram Collocations (Loose Measures)\n",
        "\n",
        "Webcast    Part      \n",
        "Proof      Concept   \n",
        "Moving     mysterious\n",
        "budget     deficits  \n",
        "\u201cEnhanced  Use\u201d      \n",
        "InformationMaking Use\u201d      \n",
        "InformationMaking \u201cEnhanced \n",
        "Use\u201d       InformationMaking\n",
        "\u201cEnhanced  InformationMaking\n",
        "Blue       Shield    \n",
        "Forum      Market    \n",
        "Post       Market    \n",
        "Active     Surveillance\n",
        "Brockelman PLC       \n",
        "Carol      Diamond   \n",
        "Cross      Shield    \n",
        "Faster     learning\u201d \n",
        "Faster     \u201crapid    \n",
        "Kristen    Brockelman\n",
        "Kristen    PLC       \n",
        "Kristen    Schermer  \n",
        "Next       Steps     \n",
        "Part       Part      \n",
        "Participant Biographies\n",
        "Schermer   Brockelman\n",
        "Schermer   PLC       \n",
        "Sciences   GlaxoSmithKline\n",
        "\u201crapid     learning\u201d \n",
        "PET        scanners  \n",
        "Falk       Auditorium\n",
        "Forum      Post      \n",
        "Recovery   Reinvestment\n",
        "Coppersmith Brockelman\n",
        "Coppersmith PLC       \n",
        "Coppersmith Schermer  \n",
        "EDT        Auditorium\n",
        "EDT        Falk      \n",
        "Kristen    Coppersmith\n",
        "Patrick    GlaxoSmithKline\n",
        "Patrick    Sciences  \n",
        "Protection Affordable\n",
        "Rick       Perry     \n",
        "cochlear   implants  \n",
        "life       expectancy\n",
        "trials     assignment\n",
        "Annual     WorkshopThird\n",
        "WorkshopThird Annual    \n",
        "Auditorium 1775      \n",
        "Carol      Markle    \n",
        "Cross      Blue      \n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 2. Run Corpus (list of all tokens in corpus) Through Bigram Collocations (Restrictive Measures) Algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find top 50 frequently occurring pairs of words which occur AT LEAST 5 times in document and within 4 words of each other\n",
      "#ignore stopwords and words of 3 chars or fewer\n",
      "findCollocsfromTokens(health_it_corpus, func=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Important Bigram Collocations (Restrictive Measures)\n",
        "\n",
        "Events     Blogs     \n",
        "Shared     Savings   \n",
        "\u201cmeaningful use\u201d      \n",
        "Events     Experts   \n",
        "Experts    Blogs     \n",
        "Markle     Foundation\n",
        "evidence-based decision-making\n",
        "Main       image     \n",
        "Blogs      Support   \n",
        "Carolyn    Clancy    \n",
        "England    Journal   \n",
        "Observational Partnership\n",
        "AuditoriumThe Institution1775\n",
        "Experts    Support   \n",
        "Map        Content   \n",
        "mp3        Download  \n",
        "Fly        Title     \n",
        "emergency  room      \n",
        "Map        Related   \n",
        "Article    article   \n",
        "Article    standard  \n",
        "Outcomes   Partnership\n",
        "Observational Outcomes  \n",
        "School     Pilgrim   \n",
        "comparative effectiveness\n",
        "networking sites     \n",
        "product    surveillance\n",
        "Closing    Remarks   \n",
        "Participants Panelists \n",
        "breast     cancer    \n",
        "vital      signs     \n",
        "Savings    Program   \n",
        "Shared     Program   \n",
        "history    signs     \n",
        "Harvard    Pilgrim   \n",
        "Ave.       NWWashington\n",
        "private    sector    \n",
        "Institution1775 Ave.      \n",
        "blood      pressure  \n",
        "standard   article   \n",
        "save       money     \n",
        "history    vital     \n",
        "Related    Content   \n",
        "Governance Studies   \n",
        "Economic   Studies   \n",
        "Annual     Initiative\n",
        "Darrell    West      \n",
        "West       Penn      \n",
        "article    Issue     \n",
        "remote     monitoring\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Step 3. Run Corpus (list of all tokens in corpus) Through Trigram Collocations Algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find top 20 frequently occurring triplets of words which occur AT LEAST twice in document; ignore stopwords and words of 3 chars or fewer\n",
      "findCollocsfromTokens(health_it_corpus, func=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Important Trigram Collocations\n",
        "\n",
        "Customer   Satisfaction Index     \n",
        "tremendously underutilized resource.\u201d\n",
        "Collaboration Legal      Guidance  \n",
        "DarrWest   View       Bio       \n",
        "Interchange Standards  Consortium\n",
        "Hampshire  BallroomRenaissance Dupont    \n",
        "Carol      Diamond    Managing  \n",
        "call       button     responses \n",
        "Harris     Interactive survey    \n",
        "InformationMaking \u201cEnhanced  Use\u201d      \n",
        "Falk       Auditorium 1775      \n",
        "Data       Interchange Standards \n",
        "institutional review     boards    \n",
        "Safety     Collaboration Legal     \n",
        "Lou        Gehrig\u2019s   disease   \n",
        "Making     \u201cEnhanced  Use\u201d      \n",
        "panelists  took       audience  \n",
        "worrisome  trends     represents\n",
        "Next       Steps      Details   \n",
        "anticoagulant drug       warfarin  \n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find top 50 frequently occurring pairs of words which occur AT LEAST 3 times in document and within 10 words of each other\n",
      "#ignore stopwords and words of 3 chars or fewer\n",
      "findCollocsfromTokens(health_lp_corpus, func=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Important Bigram Collocations (Loose Measures)\n",
        "\n",
        "Proof      Concept   \n",
        "paradigm   shifting  \n",
        "Carnegie   Endowment \n",
        "InformationMaking Use\u201d      \n",
        "InformationMaking \u201cEnhanced \n",
        "Use\u201d       InformationMaking\n",
        "dual       eligibles \n",
        "\u201cEnhanced  InformationMaking\n",
        "\u201cEnhanced  Use\u201d      \n",
        "Brookings-Bern Displacement\n",
        "Lessons    Learned   \n",
        "Lipsky     Martin    \n",
        "Post       page      \n",
        "Post       to:       \n",
        "Recovery   Reinvestment\n",
        "page       to:       \n",
        "rheumatoid arthritis \n",
        "adept      coalitions\n",
        "single-issue coalitions\n",
        "Box        speaks    \n",
        "Brookings-Bern Internal  \n",
        "Internal   Displacement\n",
        "Kavita     Patel     \n",
        "Leonard    Schaeffer \n",
        "Maxwell    Temin     \n",
        "Outside    speaks    \n",
        "Priority   Populations\n",
        "Rep.       Waxman    \n",
        "barreras   econ\u00f3micas\n",
        "justice    boycotted \n",
        "one-day    workshop  \n",
        "pickup     trucks    \n",
        "Falk       Auditorium\n",
        "Signal     Refinement\n",
        "spinal     cord      \n",
        "forming    coalitions\n",
        "imagination aside     \n",
        "Northeast  Asian     \n",
        "Sen.       Sheldon   \n",
        "Dec        Past      \n",
        "Individual Mandate   \n",
        "Rachel     BehrmanOffice\n",
        "San        Ramon     \n",
        "debt       ceiling   \n",
        "sociedad   civil     \n",
        "Image      Source    \n",
        "Annual     WorkshopThird\n",
        "Cohen      Noll      \n",
        "Kathleen   Sebelius  \n",
        "Outside    Box       \n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find top 50 frequently occurring pairs of words which occur AT LEAST 5 times in document and within 4 words of each other\n",
      "#ignore stopwords and words of 3 chars or fewer\n",
      "findCollocsfromTokens(health_lp_corpus, func=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Important Bigram Collocations (Restrictive Measures)\n",
        "\n",
        "Keynote    Address   \n",
        "Johns      Hopkins   \n",
        "accessed   Jan       \n",
        "Map        Content   \n",
        "Task       Force     \n",
        "sugary     beverages \n",
        "Events     Blogs     \n",
        "Downloads  Download  \n",
        "bone       marrow    \n",
        "intellectual property  \n",
        "Active     Surveillance\n",
        "Observational Partnership\n",
        "Patient    Protection\n",
        "signal     refinement\n",
        "nuclear    transplantation\n",
        "Active     Product   \n",
        "Henry      Aaron     \n",
        "Blogs      Support   \n",
        "employer   mandates  \n",
        "Advisory   Board     \n",
        "Independent Advisory  \n",
        "Product    Surveillance\n",
        "Map        Related   \n",
        "Social     Security  \n",
        "protecci\u00f3n financiera\n",
        "Congressional Budget    \n",
        "1775       Ave.      \n",
        "Participants Panelists \n",
        "Events     Experts   \n",
        "Experts    Blogs     \n",
        "Institution1775 Ave.      \n",
        "baby       boomers   \n",
        "...        accessed  \n",
        "Observational Outcomes  \n",
        "Outcomes   Partnership\n",
        "Related    Content   \n",
        "presidential election  \n",
        "John       Muir      \n",
        "Closing    Remarks   \n",
        "Ave.       NWWashington\n",
        "Independent Board     \n",
        "plant      animal    \n",
        "Ave.       Map       \n",
        "Experts    Support   \n",
        "Jan        2008      \n",
        "Protection Affordable\n",
        "Guest      Opinion   \n",
        "1775       Massachusetts\n",
        "Institution1775 Massachusetts\n",
        "...        Jan       \n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find top 20 frequently occurring triplets of words which occur AT LEAST twice in document; ignore stopwords and words of 3 chars or fewer\n",
      "findCollocsfromTokens(health_lp_corpus, func=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Important Trigram Collocations\n",
        "\n",
        "Metro      Center775  12th      \n",
        "\u201cSmall     business\u201d  occupies  \n",
        "Carol      Diamond    Managing  \n",
        "Personal   Dust       Monitor   \n",
        "Wood       Johnson    Foundation\u2019s\n",
        "well-maintained recreational facilities\n",
        "\u201cSmall-Business Owner      Concerns  \n",
        "Earned     Income     Tax       \n",
        "Harris     Interactive telephone \n",
        "NBC        News/Wall  Street    \n",
        "Local      Living     Economies \n",
        "pop\u201d       retail     store     \n",
        "\u201cbig       box\u201d       retail    \n",
        "Fail       Coalition  alongside \n",
        "Income     Tax        Credit    \n",
        "attorney   Joel       Marks     \n",
        "cited      \u201cimproving schools/training\n",
        "coronary   artery     bypass    \n",
        "opposes    \u201cbig       box\u201d      \n",
        "box\u201d       retail     establishments\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "______"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}