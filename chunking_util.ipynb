{
 "metadata": {
  "name": "",
  "signature": "sha256:aef3932c7403687ff4a0de836eaf6d9bfc0bc3bddc1738927eaa1f0309a0fc8a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, string\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.collocations import *\n",
      "import codecs, os, json, glob\n",
      "from bs4 import BeautifulSoup\n",
      "from nltk.corpus import wordnet as wn\n",
      "from pandas import DataFrame\n",
      "import pandas as pd\n",
      "import get_sents\n",
      "\n",
      "import file_utils\n",
      "from os import path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getPhraseLengths(chunkfd):\n",
      "    \"break chunks into groups by length\"\n",
      "    lenMore = []\n",
      "    len3 = []\n",
      "    len2 = []\n",
      "    len1 = []\n",
      "    combined = {}\n",
      "    \n",
      "    for c in chunkfd:\n",
      "        if len(c.split()) > 3:\n",
      "            lenMore.append(c)\n",
      "        if len(c.split()) == 3:\n",
      "            len3.append(c)\n",
      "        if len(c.split()) == 2:\n",
      "            len2.append(c)\n",
      "        if len(c.split()) == 1:\n",
      "            len1.append(c)\n",
      "    return len1, len2, len3, lenMore\n",
      "\n",
      "def mergeTerms(list1):\n",
      "    \"merge ngrams into same length other ngrams\"\n",
      "    results = []\n",
      "    list2 = set([i.lower() for i in list1])\n",
      "    \n",
      "    for i in list2: \n",
      "        flag = True\n",
      "        for j in list2:\n",
      "            if i in j and i != j:\n",
      "                results.append(j)\n",
      "                flag = False\n",
      "                \n",
      "        if flag == True and i not in results: \n",
      "            results.append(i)\n",
      "    \n",
      "    return set(results)\n",
      "\n",
      "\n",
      "def mergeTerms2Lists(shortlenlist, longerlenlist):\n",
      "    \"merge shorter length terms into longer length terms\"\n",
      "    termMapping = {}\n",
      "    \n",
      "    for j in longerlenlist:\n",
      "        termMapping[j] = []\n",
      "    for i in shortlenlist: \n",
      "        flag = True\n",
      "        for j in longerlenlist:\n",
      "            if i.lower() in j.lower():\n",
      "                termMapping[j].append(i)\n",
      "                flag = False\n",
      "        if flag == True:\n",
      "            termMapping[i] = []\n",
      "    \n",
      "    return termMapping"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_best_chunks(df):\n",
      "    corpus_chunks = []\n",
      "    for chunk in df['article_chunks']:\n",
      "        corpus_chunks.extend(chunk)\n",
      "\n",
      "    chunk_fd = nltk.FreqDist(corpus_chunks)\n",
      "    freqchunks = chunk_fd.keys()[:100]\n",
      "\n",
      "    #produce chunk groups by length\n",
      "    len1, len2, len3, lenMore = getPhraseLengths(freqchunks)\n",
      "\n",
      "    #condense all terms of same length into new lists\n",
      "    new_len2 = mergeTerms(len2)\n",
      "    new_len3 = mergeTerms(len3)\n",
      "    new_lenMore = mergeTerms(lenMore)\n",
      "\n",
      "    #resulting list \n",
      "    merged2_3 = mergeTerms2Lists(new_len2, new_len3)\n",
      "\n",
      "    #resulting list if we decide to merge into larger chunks\n",
      "    improved_freqchunks = mergeTerms2Lists(merged2_3.keys(), new_lenMore)\n",
      "    return improved_freqchunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chunk_mapper(df, n_chunks=100):\n",
      "    corpus_chunks = []\n",
      "    for chunk in df['article_chunks']:\n",
      "        corpus_chunks.extend(chunk)\n",
      "    \n",
      "    chunk_fd = nltk.FreqDist(corpus_chunks)\n",
      "    freqchunks = chunk_fd.keys()[:n_chunks]\n",
      "    \n",
      "    chunk_map={}\n",
      "    \n",
      "    for c in freqchunks:\n",
      "        for k,v in chunk_map.items():\n",
      "            if c.lower() in k.lower() or k.lower() in c.lower():\n",
      "                chunk_map[c]=v\n",
      "                break\n",
      "        \n",
      "        if c not in chunk_map:\n",
      "                chunk_map[c]=c\n",
      "    \n",
      "    return chunk_map"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def apply_map(row, chunk_map):\n",
      "    return set(chunk_map[c] for c in row if c in chunk_map.keys())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_best_chunk_df(df, chunk_col='article_chunks'):\n",
      "    \"extracts the best chunks from dataframe df using column chunk_col\"\n",
      "    \n",
      "    chunks = set(get_best_chunks(df))\n",
      "    return df[chunk_col].map(lambda r: chunks & set(r))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def df_chunks_from_path(fdir, expression='*[0-9].json', V=False):\n",
      "    \"reads files from path matching expression and returns df with chunk columns\"\n",
      "    \n",
      "\n",
      "        \n",
      "    df = file_utils.df_from_path(fdir, expression, V)\n",
      "    \n",
      "    if V:\n",
      "        print \"Getting best chunks\"\n",
      "        \n",
      "    df['common_chunks']=create_best_chunk_df(df)\n",
      "    \n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}